---
title: "Getting started with `casemix`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{casemix-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)
```

This vignette will demonstrate how to use the `casemix` package to nonparametrically estimate the case-mix adjusted quality of many units. A prominent application of these methods is estimating the quality of health care providers (e.g., hospitals, health insurance plans, doctors) adjusted for the mix of patients that they see. These methods are also used in education research where the units might be schools or teachers, and adjustment might be made for the mix of students taught by school/teacher. For the purposes of demonstration, we will use the terminology of providers (for units) and patients (for individual observations) throughout the rest of this demonstration.

There are two types of quality estimates generated by `casemix`: population-weighted quality estimates and equity-weighted quality estimates. Population-weighting estimates the quality of providers standardized to the population of all patients. These answer the counterfactual question of what the provider's quality would be if they saw a mix of patients similar to the population distribution of patients. Equity-weighting estimates quality standardized to a different population defined by certain equity features. Unlike population-weighted quality, which can be determined mostly by the largest groups of patients, equity-weighting ensures that certain groups of patients (defined by, e.g., race, sex, income, etc.) are equally represented in the quality measurement. 

Note that the package relies on the mlr3 universe of packages to standardize prediction tasks for nuisance function estimation. The learners available to the user can be seen at https://mlr3learners.mlr-org.com/, and additional learners can be accessed from the Github package `mlr3extralearners` at https://github.com/mlr-org/mlr3extralearners.

## Preliminaries

First, we load a sample dataset and a dataset that indicates the true underlying quality of each provider. In the dataset, `id` is a random patient ID, `unit` indicates the provider, `y` indicates the binary measure of interest, `w1`, `w2`, `w3` are characteristics on which we might wish to ensure equity (e.g., race, sex, income) and `x1`, `x2`, `x3` are other characteristics that we wish to control for. This data comes loaded with the package, and the `simulate_data` function can be used to generate similar data.

```{r setup}
library(casemix)
library(dplyr)
library(mlr3)
library(mlr3learners)

data(sample_dataset)
data(true_quality)
```

We can first look at the true population-weighted quality for each of the ten providers

```{r}
knitr::kable(true_quality %>%
               select(unit, popwt_quality) %>%
               arrange(as.numeric(unit)), digits = 3)
```

We can also see the results we would obtain if we did not adjust for patient mix:

```{r}
knitr::kable(sample_dataset %>%
  group_by(unit) %>%
  summarise(unadjusted_quality = mean(y)) %>%
    arrange(as.numeric(unit)), digits = 3)
```

## Population-weighted quality

If there are no particular equity concerns, but one only wishes to estimate provider quality nonparametrically, there are two population-weighted functions: `popwt_plugin`, which estimates a plug-in estimator of quality, and `popwt_tmle`, which estimates a targeted-minimum-loss-based estimate of quality. The usage for the two functions is similar. The dataset to be used is indicated in the `data` argument, and the columns in the dataset representing observation ID (`id`), the unit identifier (`a`), the outcome (`y`), and covariates to adjust for (`covars`) are indicated with strings.

The output returns the estimates with associated standard errors, as well as shrinkage versions of the estimates under a normal means model. The reliability of the estimate, which measures the relative variance of quality among providers compared to the total variability in the dataset, is also reported. The plug-in results look like this:

```{r}
pop_plugin_est <- popwt_plugin(data = sample_dataset, id = 'id', a = 'unit', y = 'y', covars = c('w1', 'w2', 'w3', 'x1', 'x2', 'x3'), lrnr = lrn('classif.ranger'))
knitr::kable(pop_plugin_est %>%
    arrange(as.numeric(unit)), digits = 3)
```

The targeted minimum loss-based estimation usage and results are similar:

```{r}
pop_tml_est <- popwt_tmle(data = sample_dataset, id = 'id', a = 'unit', y = 'y', covars = c('w1', 'w2', 'w3', 'x1', 'x2', 'x3'), lrnr = lrn('classif.ranger'))
knitr::kable(pop_tml_est %>%
    arrange(as.numeric(unit)), digits = 3)
```

### Nuisance function estimation 

These estimators require the estimation of two types of nuisance functions: ones that model the differences between units (propensity scores) and ones that model the relationship between patient characteristics, providers, and the outcome (outcome functions). By default, random forests are used for both types of estimation. One can specify the same learner for all nuisance functions by default through the `lrnr` argument, or separate propensity score (`lrnr_e`) and outcome function (`lrnr_mu`) learners. The `mlr3` package is used for learner specification. Here is an example where multinomial logistic regression is used for propensity scores and standard binary logistic regression is used for the outcome function estimation:

```{r}
pop_plugin_logistic <- popwt_plugin(data = sample_dataset, id = 'id', a = 'unit', y = 'y', covars = c('w1', 'w2', 'w3', 'x1', 'x2', 'x3'), lrnr_e = mlr3::lrn('classif.multinom'), lrnr_mu = mlr3::lrn('classif.log_reg'))
knitr::kable(pop_plugin_logistic %>%
    arrange(as.numeric(unit)), digits = 3)
```

Here's another example using k-nearest neighbors for both nuisance functions.

```{r}
pop_plugin_knn <- popwt_plugin(data = sample_dataset, id = 'id', a = 'unit', y = 'y', covars = c('w1', 'w2', 'w3', 'x1', 'x2', 'x3'), lrnr = mlr3::lrn('classif.kknn'))
knitr::kable(pop_plugin_knn %>%
    arrange(as.numeric(unit)), digits = 3)
```

### Equity-weighted quality

The equity-weighted versions of these functions estimate quality in a population of patients where each group defined by the characteristics in `wvars` is given equal weight. In this case, we have three such characteristics:

```{r}
sample_dataset %>%
  count(w1)

sample_dataset %>%
  count(w2)

sample_dataset %>%
  count(w3)
```

The equity-weighted estimators ensure that those with `w3` = 1 are given the same weight as those with `w3` = 0, whereas the population-weighted quality estimates give about 80% weight to those with `w3` = 1. Similarly, those with `w2` = 1 and `w2` = 0 are given the same weight. The usage of these functions is very similar to the population-weighted estimators above, but now one must identify the equity characteristics in the argument `wvars` and the other characteristics to control for in 

```{r}
eq_plugin_est <- eqwt_plugin(data = sample_dataset, id = 'id', a = 'unit', y = 'y', wvars = c('w1', 'w2', 'w3'), zvars = c('x1', 'x2', 'x3'), lrnr = lrn('classif.ranger'))
knitr::kable(eq_plugin_est %>% select(-wt_ds) %>%
    arrange(as.numeric(unit)), digits = 3)
```

The equity-weighted results also include the weights, so users can reason about the equity-weighted population:

```{r}
knitr::kable(eq_plugin_est$wt_ds[[1]], digits = 3)
```

### Notes about sample splitting

All of the functions use sample splitting to ensure that nuisance function estimation (outcome functions and propensity scores) are done on separate samples from the samples on which they are evaluated. However, this means that estimation can be sensitive to the choice of splits. Users can either split the samples themselves using the `make_folds` function. See how the column `fold` is added to the sample dataset using the following code:

```{r}

sample_dataset_wfolds <- make_folds(sample_dataset, unit_id = 'unit', K = 2)
head(sample_dataset_wfolds)
```

Or, alternatively, it is best practice to set the random seed before running any of the package functions to ensure that sample splitting is replicable.

One final note: it may be advisable to run the function multiple time with different sample splits to ensure that results are not overly sensitive to the particular sample split.
